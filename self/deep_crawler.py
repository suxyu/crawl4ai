#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Deep Crawler for KKDaxue Domain
ä½¿ç”¨ crawl4ai çš„æ·±åº¦çˆ¬å–åŠŸèƒ½çˆ¬å– kkdaxue.com åŸŸåä¸‹çš„æ‰€æœ‰ç½‘é¡µ
"""

import asyncio
import sys
import subprocess
import os
import time
import re
from pathlib import Path
from typing import List, Dict, Any
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.deep_crawling import BFSDeepCrawlStrategy, DFSDeepCrawlStrategy
from crawl4ai.content_scraping_strategy import LXMLWebScrapingStrategy


class DeepCrawler:
    """æ·±åº¦çˆ¬è™«ç±»"""
    
    def __init__(self, domain: str = "kkdaxue.com", headless: bool = False):
        self.domain = domain
        self.base_url = f"https://www.{domain}"
        self.crawled_urls = set()
        self.power_crawler_path = "power_crawler.py"
        self.start_time = time.time()
        self.headless = headless
        self.output_dir = f"deep_crawl_output_{self.domain.replace('.', '_')}"
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.output_dir, exist_ok=True)
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        
    def extract_urls_from_html(self, html_content: str) -> List[str]:
        """
        ä»HTMLå†…å®¹ä¸­æå–æ‰€æœ‰é“¾æ¥
        
        Args:
            html_content: HTMLå†…å®¹
            
        Returns:
            URLåˆ—è¡¨
        """
        urls = set()
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–é“¾æ¥
        # åŒ¹é… href å±æ€§
        href_pattern = r'href=["\']([^"\']+)["\']'
        href_matches = re.findall(href_pattern, html_content)
        
        # åŒ¹é… src å±æ€§
        src_pattern = r'src=["\']([^"\']+)["\']'
        src_matches = re.findall(src_pattern, html_content)
        
        # åˆå¹¶æ‰€æœ‰åŒ¹é…
        all_matches = href_matches + src_matches
        
        for match in all_matches:
            # å¤„ç†ç›¸å¯¹URL
            if match.startswith('/'):
                url = f"https://www.{self.domain}{match}"
                urls.add(url)
            elif match.startswith('http') and self.domain in match:
                urls.add(match)
            elif not match.startswith(('http', 'javascript:', 'mailto:', 'tel:')):
                # ç›¸å¯¹è·¯å¾„
                if match and not match.startswith('#'):
                    url = f"https://www.{self.domain}/{match.lstrip('/')}"
                    urls.add(url)
        
        return list(urls)
    
    async def discover_all_urls(self, max_depth: int = 3, max_pages: int = 100) -> List[str]:
        """
        å‘ç°åŸŸåä¸‹çš„æ‰€æœ‰ URL
        
        Args:
            max_depth: çˆ¬å–æ·±åº¦
            max_pages: æœ€å¤§é¡µé¢æ•°é‡
            
        Returns:
            å‘ç°çš„ URL åˆ—è¡¨
        """
        print(f"å¼€å§‹å‘ç° {self.domain} åŸŸåä¸‹çš„æ‰€æœ‰ URL...")
        print(f"çˆ¬å–æ·±åº¦: {max_depth}, æœ€å¤§é¡µé¢æ•°: {max_pages}")
        print(f"æµè§ˆå™¨æ¨¡å¼: {'æœ‰å¤´æ¨¡å¼' if not self.headless else 'æ— å¤´æ¨¡å¼'}")
        
        try:
            async with AsyncWebCrawler() as crawler:
                # é…ç½®æ·±åº¦çˆ¬å–ç­–ç•¥
                config = CrawlerRunConfig(
                    deep_crawl_strategy=BFSDeepCrawlStrategy(
                        max_depth=max_depth,
                        include_external=False,  # åªçˆ¬å–åŒä¸€åŸŸå
                        max_pages=max_pages,
                        score_threshold=0.1  # è¾ƒä½çš„é˜ˆå€¼ï¼Œç¡®ä¿è·å–æ›´å¤š URL
                    ),
                    scraping_strategy=LXMLWebScrapingStrategy(),
                    verbose=True
                )
                
                print("å¼€å§‹æ·±åº¦çˆ¬å–...")
                results = await crawler.arun(self.base_url, config=config)
                
                if not results:
                    print("æ·±åº¦çˆ¬å–å¤±è´¥")
                    return []
                
                print(f"æ·±åº¦çˆ¬å–å®Œæˆï¼Œå…±å‘ç° {len(results)} ä¸ªé¡µé¢")
                
                # æå–æ‰€æœ‰ URL
                all_urls = set()
                
                # ä»æ·±åº¦çˆ¬å–ç»“æœä¸­æå–URL
                for result in results:
                    if hasattr(result, 'url') and result.url:
                        all_urls.add(result.url)
                        print(f"å‘ç°é¡µé¢: {result.url}")
                    
                    # ä»HTMLå†…å®¹ä¸­æå–æ›´å¤šé“¾æ¥
                    if hasattr(result, 'html') and result.html:
                        extracted_urls = self.extract_urls_from_html(result.html)
                        print(f"ä» {result.url} æå–åˆ° {len(extracted_urls)} ä¸ªé“¾æ¥")
                        all_urls.update(extracted_urls)
                
                # æ‰‹åŠ¨æ·»åŠ ä¸€äº›å·²çŸ¥çš„URLæ¨¡å¼
                print("æ·»åŠ å·²çŸ¥çš„URLæ¨¡å¼...")
                known_patterns = [
                    f"https://www.{self.domain}/post/add",
                    f"https://www.{self.domain}/?current=1",
                    f"https://www.{self.domain}/?current=2",
                    f"https://www.{self.domain}/?current=3",
                    f"https://www.{self.domain}/?current=4",
                    f"https://www.{self.domain}/?current=5",
                    f"https://www.{self.domain}/?current=10",
                    f"https://www.{self.domain}/?current=20",
                    f"https://www.{self.domain}/?current=50",
                    f"https://www.{self.domain}/?current=100",
                ]
                all_urls.update(known_patterns)
                
                # å»é‡å¹¶è¿‡æ»¤
                filtered_urls = []
                for url in all_urls:
                    if (self.domain in url and 
                        url.startswith('http') and 
                        url not in self.crawled_urls and
                        not url.endswith(('.jpg', '.jpeg', '.png', '.gif', '.css', '.js'))):
                        filtered_urls.append(url)
                
                print(f"è¿‡æ»¤åå…±æœ‰ {len(filtered_urls)} ä¸ªå”¯ä¸€ URL")
                
                # æ˜¾ç¤ºå‰20ä¸ªURLç”¨äºè°ƒè¯•
                print("å‰20ä¸ªå‘ç°çš„URL:")
                for i, url in enumerate(filtered_urls[:20], 1):
                    print(f"  {i}. {url}")
                
                return filtered_urls
                
        except Exception as e:
            print(f"æ·±åº¦çˆ¬å–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    async def crawl_single_page_with_power_crawler(self, url: str) -> bool:
        """
        ä½¿ç”¨ power_crawler.py çˆ¬å–å•ä¸ªé¡µé¢
        
        Args:
            url: è¦çˆ¬å–çš„ URL
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            print(f"ä½¿ç”¨ power_crawler.py çˆ¬å–: {url}")
            
            # æ£€æŸ¥ power_crawler.py æ˜¯å¦å­˜åœ¨
            if not os.path.exists(self.power_crawler_path):
                print(f"é”™è¯¯: {self.power_crawler_path} æ–‡ä»¶ä¸å­˜åœ¨")
                return False
            
            # è°ƒç”¨ power_crawler.pyï¼Œä¼ é€’headlesså‚æ•°
            cmd = [sys.executable, self.power_crawler_path, url]
            if not self.headless:
                cmd.append("--visible")
            
            result = subprocess.run(cmd, capture_output=True, text=True, cwd=os.getcwd())
            
            if result.returncode == 0:
                print(f"âœ… æˆåŠŸçˆ¬å–: {url}")
                self.crawled_urls.add(url)
                
                # æŸ¥æ‰¾å¹¶å¤åˆ¶ç”Ÿæˆçš„æ–‡ä»¶åˆ°è¾“å‡ºç›®å½•
                self.copy_generated_files(url)
                
                return True
            else:
                print(f"âŒ çˆ¬å–å¤±è´¥: {url}")
                print(f"é”™è¯¯ä¿¡æ¯: {result.stderr}")
                return False
                
        except Exception as e:
            print(f"è°ƒç”¨ power_crawler.py å¤±è´¥: {e}")
            return False
    
    def copy_generated_files(self, url: str):
        """å¤åˆ¶ç”Ÿæˆçš„æ–‡ä»¶åˆ°è¾“å‡ºç›®å½•"""
        try:
            # ç”Ÿæˆå¯èƒ½çš„æ–‡ä»¶å
            domain_part = url.split('//')[-1].split('/')[0].replace('.', '_')
            path_part = url.split('//')[-1].split('/', 1)[1] if '/' in url.split('//')[-1] else ''
            if path_part:
                path_part = path_part.replace('?', '_').replace('&', '_').replace('=', '_')
            
            possible_files = [
                f"fully_expanded_{domain_part}.md",
                f"power_crawled_{domain_part}.md",
                f"expanded_{domain_part}.md",
                f"crawled_{domain_part}.txt"
            ]
            
            for filename in possible_files:
                if os.path.exists(filename):
                    # åˆ›å»ºæ–°çš„æ–‡ä»¶å
                    new_filename = f"page_{len(self.crawled_urls):03d}_{path_part or 'home'}.md"
                    if filename.endswith('.txt'):
                        new_filename = new_filename.replace('.md', '.txt')
                    
                    # å¤åˆ¶æ–‡ä»¶
                    import shutil
                    shutil.copy2(filename, os.path.join(self.output_dir, new_filename))
                    print(f"  ğŸ“ ä¿å­˜å†…å®¹åˆ°: {self.output_dir}/{new_filename}")
                    
        except Exception as e:
            print(f"å¤åˆ¶æ–‡ä»¶å¤±è´¥: {e}")
    
    async def crawl_all_pages(self, urls: List[str], delay: float = 2.0) -> Dict[str, bool]:
        """
        çˆ¬å–æ‰€æœ‰é¡µé¢
        
        Args:
            urls: URL åˆ—è¡¨
            delay: é¡µé¢é—´å»¶è¿Ÿï¼ˆç§’ï¼‰
            
        Returns:
            çˆ¬å–ç»“æœå­—å…¸ {url: success}
        """
        print(f"å¼€å§‹çˆ¬å– {len(urls)} ä¸ªé¡µé¢...")
        
        results = {}
        total = len(urls)
        
        for i, url in enumerate(urls, 1):
            print(f"\nè¿›åº¦: {i}/{total} ({i/total*100:.1f}%)")
            print(f"å½“å‰çˆ¬å–: {url}")
            
            success = await self.crawl_single_page_with_power_crawler(url)
            results[url] = success
            
            # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
            if i < total:
                print(f"ç­‰å¾… {delay} ç§’...")
                await asyncio.sleep(delay)
        
        return results
    
    def generate_summary_report(self, results: Dict[str, bool], output_file: str = "deep_crawl_report.md"):
        """
        ç”Ÿæˆçˆ¬å–æ‘˜è¦æŠ¥å‘Š
        
        Args:
            results: çˆ¬å–ç»“æœå­—å…¸
            output_file: è¾“å‡ºæ–‡ä»¶å
        """
        print(f"\nç”Ÿæˆçˆ¬å–æ‘˜è¦æŠ¥å‘Š: {output_file}")
        
        total_urls = len(results)
        successful_urls = sum(1 for success in results.values() if success)
        failed_urls = total_urls - successful_urls
        elapsed_time = time.time() - self.start_time
        
        # ä¿å­˜åˆ°è¾“å‡ºç›®å½•
        output_path = os.path.join(self.output_dir, output_file)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(f"# {self.domain} æ·±åº¦çˆ¬å–æŠ¥å‘Š\n\n")
            f.write(f"**çˆ¬å–æ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(self.start_time))}\n")
            f.write(f"**è€—æ—¶**: {elapsed_time:.2f} ç§’\n")
            f.write(f"**æµè§ˆå™¨æ¨¡å¼**: {'æœ‰å¤´æ¨¡å¼' if not self.headless else 'æ— å¤´æ¨¡å¼'}\n")
            f.write(f"**è¾“å‡ºç›®å½•**: {self.output_dir}\n")
            f.write(f"**æ€» URL æ•°**: {total_urls}\n")
            f.write(f"**æˆåŠŸçˆ¬å–**: {successful_urls}\n")
            f.write(f"**å¤±è´¥æ•°é‡**: {failed_urls}\n")
            f.write(f"**æˆåŠŸç‡**: {successful_urls/total_urls*100:.1f}%\n\n")
            
            f.write("## çˆ¬å–è¯¦æƒ…\n\n")
            f.write("| åºå· | URL | çŠ¶æ€ |\n")
            f.write("|------|-----|------|\n")
            
            for i, (url, success) in enumerate(results.items(), 1):
                status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
                f.write(f"| {i} | {url} | {status} |\n")
            
            f.write("\n## æˆåŠŸçˆ¬å–çš„é¡µé¢\n\n")
            for url, success in results.items():
                if success:
                    f.write(f"- {url}\n")
            
            f.write("\n## å¤±è´¥çš„é¡µé¢\n\n")
            for url, success in results.items():
                if not success:
                    f.write(f"- {url}\n")
            
            f.write(f"\n## è¾“å‡ºæ–‡ä»¶\n\n")
            f.write(f"æ‰€æœ‰çˆ¬å–çš„å†…å®¹éƒ½ä¿å­˜åœ¨ `{self.output_dir}` ç›®å½•ä¸­ã€‚\n")
            
            # åˆ—å‡ºè¾“å‡ºç›®å½•ä¸­çš„æ–‡ä»¶
            if os.path.exists(self.output_dir):
                files = os.listdir(self.output_dir)
                if files:
                    f.write("\n**ç”Ÿæˆçš„æ–‡ä»¶:**\n")
                    for file in sorted(files):
                        f.write(f"- {file}\n")
        
        print(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
        print(f"çˆ¬å–å®Œæˆ! æˆåŠŸ: {successful_urls}/{total_urls}")
        print(f"æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
        print(f"æ‰€æœ‰å†…å®¹å·²ä¿å­˜åˆ°: {self.output_dir}")
    
    async def run_full_crawl(self, max_depth: int = 3, max_pages: int = 100, delay: float = 2.0):
        """
        è¿è¡Œå®Œæ•´çš„æ·±åº¦çˆ¬å–æµç¨‹
        
        Args:
            max_depth: çˆ¬å–æ·±åº¦
            max_pages: æœ€å¤§é¡µé¢æ•°é‡
            delay: é¡µé¢é—´å»¶è¿Ÿ
        """
        print("=" * 60)
        print(f"å¼€å§‹ {self.domain} çš„å®Œæ•´æ·±åº¦çˆ¬å–")
        print(f"æµè§ˆå™¨æ¨¡å¼: {'æœ‰å¤´æ¨¡å¼' if not self.headless else 'æ— å¤´æ¨¡å¼'}")
        print("=" * 60)
        
        # ç¬¬ä¸€æ­¥ï¼šå‘ç°æ‰€æœ‰ URL
        urls = await self.discover_all_urls(max_depth, max_pages)
        
        if not urls:
            print("æ²¡æœ‰å‘ç°ä»»ä½• URLï¼Œçˆ¬å–ç»“æŸ")
            return
        
        print(f"\nå‘ç° {len(urls)} ä¸ª URL:")
        for i, url in enumerate(urls[:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"  {i}. {url}")
        if len(urls) > 10:
            print(f"  ... è¿˜æœ‰ {len(urls) - 10} ä¸ª URL")
        
        # ç¬¬äºŒæ­¥ï¼šçˆ¬å–æ‰€æœ‰é¡µé¢
        results = await self.crawl_all_pages(urls, delay)
        
        # ç¬¬ä¸‰æ­¥ï¼šç”ŸæˆæŠ¥å‘Š
        self.generate_summary_report(results)
        
        print("\n" + "=" * 60)
        print("æ·±åº¦çˆ¬å–å®Œæˆ!")
        print(f"æ‰€æœ‰å†…å®¹å·²ä¿å­˜åˆ°: {self.output_dir}")
        print("=" * 60)


async def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) == 1:
        print("ä½¿ç”¨æ–¹æ³•: python deep_crawler.py [--depth N] [--max-pages N] [--delay N] [--visible]")
        print("å‚æ•°è¯´æ˜:")
        print("  --depth N        çˆ¬å–æ·±åº¦ (é»˜è®¤: 3)")
        print("  --max-pages N    æœ€å¤§é¡µé¢æ•° (é»˜è®¤: 100)")
        print("  --delay N        é¡µé¢é—´å»¶è¿Ÿç§’æ•° (é»˜è®¤: 2)")
        print("  --visible        æ˜¾ç¤ºæµè§ˆå™¨çª—å£ (é»˜è®¤: æ— å¤´æ¨¡å¼)")
        print("ç¤ºä¾‹: python deep_crawler.py --depth 2 --max-pages 50 --delay 3 --visible")
        print("\næ³¨æ„: ç¡®ä¿ power_crawler.py åœ¨åŒä¸€ç›®å½•ä¸‹")
        return
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    max_depth = 3
    max_pages = 100
    delay = 2.0
    headless = True  # é»˜è®¤æ— å¤´æ¨¡å¼
    
    i = 1
    while i < len(sys.argv):
        if sys.argv[i] == "--depth" and i + 1 < len(sys.argv):
            try:
                max_depth = int(sys.argv[i + 1])
                i += 2
            except ValueError:
                print(f"é”™è¯¯: --depth å‚æ•°å¿…é¡»æ˜¯æ•´æ•°ï¼Œæ”¶åˆ°: {sys.argv[i + 1]}")
                return
        elif sys.argv[i] == "--max-pages" and i + 1 < len(sys.argv):
            try:
                max_pages = int(sys.argv[i + 1])
                i += 2
            except ValueError:
                print(f"é”™è¯¯: --max-pages å‚æ•°å¿…é¡»æ˜¯æ•´æ•°ï¼Œæ”¶åˆ°: {sys.argv[i + 1]}")
                return
        elif sys.argv[i] == "--delay" and i + 1 < len(sys.argv):
            try:
                delay = float(sys.argv[i + 1])
                i += 2
            except ValueError:
                print(f"é”™è¯¯: --delay å‚æ•°å¿…é¡»æ˜¯æ•°å­—ï¼Œæ”¶åˆ°: {sys.argv[i + 1]}")
                return
        elif sys.argv[i] == "--visible":
            headless = False
            i += 1
        else:
            i += 1
    
    print(f"é…ç½®å‚æ•°:")
    print(f"  çˆ¬å–æ·±åº¦: {max_depth}")
    print(f"  æœ€å¤§é¡µé¢æ•°: {max_pages}")
    print(f"  é¡µé¢é—´å»¶è¿Ÿ: {delay} ç§’")
    print(f"  æµè§ˆå™¨æ¨¡å¼: {'æœ‰å¤´æ¨¡å¼' if not headless else 'æ— å¤´æ¨¡å¼'}")
    
    # æ£€æŸ¥ power_crawler.py æ˜¯å¦å­˜åœ¨
    if not os.path.exists("power_crawler.py"):
        print("é”™è¯¯: power_crawler.py æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·ç¡®ä¿å®ƒåœ¨åŒä¸€ç›®å½•ä¸‹")
        return
    
    # åˆ›å»ºæ·±åº¦çˆ¬è™«å®ä¾‹
    crawler = DeepCrawler(headless=headless)
    
    # è¿è¡Œå®Œæ•´çˆ¬å–
    await crawler.run_full_crawl(max_depth, max_pages, delay)


if __name__ == "__main__":
    asyncio.run(main())
